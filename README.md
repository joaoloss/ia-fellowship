# AI Fellowship Data Repository

- [AI Fellowship Data Repository](#ai-fellowship-data-repository)
  - [üìù Descri√ß√£o do problema](#-descri√ß√£o-do-problema)
  - [üíª Stack](#-stack)
  - [üí° Estrat√©gia](#-estrat√©gia)
    - [ü§ñ LLM: reduzindo lat√™ncia e custos](#-llm-reduzindo-lat√™ncia-e-custos)
    - [ü§Ø Heur√≠stica](#-heur√≠stica)
      - [Pressupostos adotados](#pressupostos-adotados)
      - [Cache](#cache)
      - [Workflow](#workflow)
    - [üí¨ Altern√¢ncia de prompt](#-altern√¢ncia-de-prompt)
  - [üë®üèª‚Äçüíª Como usar](#-como-usar)
  - [üî¢ Entrada e sa√≠da](#-entrada-e-sa√≠da)
    - [Entrada](#entrada)
    - [Sa√≠da](#sa√≠da)
  - [üß© Melhorias e limita√ß√µes reconhecidas](#-melhorias-e-limita√ß√µes-reconhecidas)


Esse reposit√≥rio cont√©m um projeto desenvolvido durante o processo seletivo para o fellowship promovido pela empresa [Enter](https://www.getenter.ai/).

## üìù Descri√ß√£o do problema

O desafio proposto gira em torno do problema de extra√ß√£o eficiente de pares chave-valor a partir de documentos desestruturados. Conforme apontado por [esse paper](https://arxiv.org/abs/2405.00505), por exemplo, *Key-Value Pair Extraction* √© uma tarefa cr√≠tica cuja solu√ß√£o eficiente permanece em aberto.

## üíª Stack

Por quest√µes de familiaridade e agilidade no desenvolvimento/prototipa√ß√£o, optou-se pela linguagem Python.

Al√©m disso, como modelo de linguagem (LLM), utilizou-se o [gpt-5-mini](https://platform.openai.com/docs/models/gpt-5-mini), da OpenAI.

## üí° Estrat√©gia

### ü§ñ LLM: reduzindo lat√™ncia e custos

**Nota**: no contexto de infer√™ncia de modelos de linguagem: `tokens consumidos = custo`. Portanto, um aumento/redu√ß√£o no n√∫mero de tokens implica um aumento/redu√ß√£o proporcional no custo final.

Sabendo que a intera√ß√£o com um LLM seria uma pe√ßa fundamental e inegoci√°vel, o primeiro passo tomado durante o desenvolvimento foi testar formas de diminuir custo e lat√™ncia (uma vez que chamadas a LLMs costumam ser o gargalo operacional e financeiro da opera√ß√£o):

1. Percebendo que a tarefa de identificar pares chave-valor n√£o demanda uma linha de racioc√≠nio muito elaborada, o primeiro teste feito foi retirar (ou, praticamente retirar) a feature de `reasoning` do modelo, setando `reasoning={"effort": "minimal"}` (os testes foram feitos passando o PDF como texto via prompt). 
    - Resultado: de ~20s para ~3s (**7x menos**) e de ~1600 tokens totais para ~400 (**4x menos**), sendo que o resultado permaneceu satisfat√≥rio.
    - Obs.: quando `effort` n√£o √© especificado, o valor padr√£o √© `medium`.

2. Para evitar formatos de sa√≠da indesejados (o que geraria problemas desnecess√°rios de JSON parsing), utilizou-se a feature de [Structured model outputs](https://platform.openai.com/docs/guides/structured-outputs), garantindo que o modelo sempre responderia conforme o modelo JSON estabelecido (utilizando a lib. `pydantic`).

3. Como uma tentativa de "enguxar" ainda mais o prompt, o esquema de entrada foi passado na estrutura YAML, que utiliza menos tokens que o formato JSON - o resultado n√£o foi significativo, um vez que essa √© um estrt√©gia cr√≠tica para cen√°rios onde o JSON passado no prompt √© extremamente longo, o que n√£o √© o caso m√©dio do desafio.

4. Para aproveitar melhor o [Prompt caching](https://platform.openai.com/docs/guides/prompt-caching) (reduzindo custo e lat√™ncia), o prompt foi organizado de forma que as se√ß√µes est√°veis permane√ßam no in√≠cio, enquanto as partes vari√°veis s√£o colocadas ao final, reduzindo a quantidade de conte√∫do que precisa ser recarregado a cada requisi√ß√£o.

5. Por fim, testou-se passar o PDF de entrada de duas formas:
    1. Utilizando a feature de [File inputs](https://platform.openai.com/docs/guides/pdf-files?api-mode=responses) via base64, o que inevitavelmente aumenta custo e lat√™ncia - uma vez que: "To help models understand PDF content, we put into the model's context both extracted text and an image of each page‚Äîregardless of whether the page includes images.", OpenAI.
    2. Utilizando apenas texto via engenharia de prompt. Realizar isso √© complicado, uma vez que o layout desempenha um papel fundamental. Para contornar esse problema foi fornecido ao modelo um esquema que lhe permite entender o layout do arquivo original (aqui, come√ßa a entrar a heur√≠stica utilizada, que ser√° detalhada no pr√≥ximo t√≥pico) atrav√©s de uma matriz. Exemplo para o arquivo `oab_1.pdf`:
        ```none
        Row 1: joana d'arc
        Row 2: inscri√ß√£o | seccional | subse√ß√£o
        Row 3: 101943 | pr | conselho seccional - paran√°
        Row 4: suplementar
        Row 5: endere√ßo profissional
        Row 6: avenida paulista, n¬∫ 2300 andar pilotis, bela vista
        Row 7: s√£o paulo - sp
        Row 8: 01310300
        Row 9: telefone profissional
        Row 10: situa√ß√£o regular
        ```
        Apesar de modelos de linguagem serem, em ess√™ncia, orientados a texto e n√£o apresentarem desempenho ideal em dados tabulares, observou-se uma melhora significativa nos resultados quando as informa√ß√µes foram estruturadas em tabela/matriz, em compara√ß√£o ao uso do texto corrido sozinho. Obviamente isso acabou resultando em um pequeno aumento de lat√™ncia e tokens consumidos.
    
    **Resultados**: enviar o arquivo PDF para o LLM (via base64), em vez do texto extra√≠do do PDF no prompt, resultou em aproximadamente **2x mais tempo** e **2x mais tokens**. Contudo, durante os experimentos, percebeu-se que, quando usando apenas texto, os resultados foram um pouco inferiores e menos consistentes. Exemplos:
    - Para a chave `"situacao"` dentro de `"label": "carteira_oab"`: em alguns casos, o modelo retornou apenas `"regular"`, enquanto em outros retornou `"situa√ß√£o regular"`. Al√©m disso, para a chave `"endereco_profissional"` dentro da mesma categoria: partes finais do endere√ßo foram ocasionalmente omitidas ‚Äî como, por exemplo, o CEP.

    Os t√≥picos a seguir apresentam a abordagem adotada para lidar com esses problemas.

### ü§Ø Heur√≠stica

#### Pressupostos adotados

1. Conjunto definido de layouts por label.
    - Assumiu-se que documentos com mesma label tendem a possuir um conjunto de layouts padr√£o. Ou seja, para uma mesma label existe um conjunto de configura√ß√µes a partir das quais os dados est√£o dispostos.
2. Mesma chave, mesmo tipo.
    - Assumiu-se valores de labels e chaves iguais possuem o mesmo tipo/formato.
    - Exemplo: dada uma label, uma chave `nome` sempre conter√° uma string, uma chave `data` sempre conter√° um valor no formato de data, uma chave `valor_total` sempre conter√° um valor num√©rio, etc..
3. LLM acerta.
    - Assume-se que o resultados gerados pela LLM (principalmente quando alimentada com o arquivo PDF nativo) est√° correto.

#### Cache

A cache √© um dicion√°rio cujos valores s√£o preenchidos de forma adaptativa ao longo do processamento dos PDFs. Sua estrutura segue tr√™s n√≠veis:

1. **N√≠vel 1**: chaves correspondendo √†s *labels* dos documentos (ex.: `carteira_oab`, `tela_sistema`, etc.), permitindo que heur√≠sticas sejam especializadas por tipo de documento.

2. **N√≠vel 2**: cada label possui um dicion√°rio como valor, cujas chaves correspondem √†s *keys* do esquema.

3. **N√≠vel 3**: cada key possui um dicion√°rio como valor, cujas chaves s√£o:
    1. `count`, que armazena a quantidade total de vezes que a key foi solicitada em um esquema de requisi√ß√£o,
    2. `heuristics`, que corresponde a uma lista de heur√≠sticas aprendidas (a ideia √© que cada heur√≠stica seja √∫til para um layout espec√≠fico),
    3. `type`, que corresponde ao tipo predominante do valor correspondente e
    4. `example_values`, que corresponde a uma lista de valores pr√©vios.

4. **N√≠vel 4**: cada heur√≠stica √© um dicion√°rio cujas chaves s√£o:
    1. `position`: posi√ß√£o do valor na representa√ß√£o matricial do conte√∫do do PDF (ver m√≥dulo `utils.pdf2mat.py`),
    2. `match_count`: n√∫mero de vezes que essa heur√≠stica foi usada,
    3. Se o tipo for `string`, h√° tamb√©m a chave `mean_length`: armazena um float com o tamanho m√©dio acumulado dos valores da chave.

    Exemplo da estrutura da cache:
    ```json
    "carteira_oab": {
        "nome": {
            "count": 3,
            "heuristics": [
                {
                    "position": [
                        0
                    ],
                    "match_count": 3,
                    "mean_length": 11
                }
            ],
            "type": "string",
            "example_values": [
                "joana d'arc",
                "luis filipe araujo amaral",
                "son goku"
            ]
        },
        "inscricao": {
            "count": 3,
            "heuristics": [
                {
                    "position": [
                        2,
                        0
                    ],
                    "match_count": 3
                }
            ],
            "type": "number",
            "example_values": [
                "101943"
            ]
        }
    }
    ```

**Antes de realizar a chamada ao modelo** executa-se um pr√©-processamento por meio do m√©todo `heuristic_preprocessing()`. Esse m√©todo utiliza a cache de heur√≠sticas j√° aprendidas para tentar preencher automaticamente parte do esquema de extra√ß√£o (`request_schema`) antes da infer√™ncia. Para cada chave do esquema, o m√©todo verifica se existem heur√≠sticas previamente armazenadas para a label do documento atual e, se existir, tenta recuperar o valor correspondente consultando diretamente a matriz do PDF. Os valores recuperados s√£o armazenados em um dicion√°rio parcial (`partial_result`), que representa os campos resolvidos apenas por heur√≠stica, sem consulta ao modelo. Durante esse processo, o m√©todo tamb√©m ajusta contadores internos e estat√≠sticas de uso das heur√≠sticas, refor√ßando aquelas que se mostram mais eficazes.

**Ap√≥s a infer√™ncia do modelo**, o m√©todo `heuristic_update()` √© respons√°vel por atualizar a cache com os novos resultados obtidos. Ele registra o valor retornado, determina seu tipo, coleta exemplos representativos e identifica a posi√ß√£o do valor no PDF, transformando esse conhecimento em novas heur√≠sticas. Se uma heur√≠stica existente j√° corresponder ao valor observado, sua frequ√™ncia de acerto √© incrementada; caso contr√°rio, uma nova heur√≠stica √© adicionada. O conjunto √© ent√£o reordenado para priorizar heur√≠sticas mais consistentes, mantendo apenas as mais relevantes para uso futuro.

Em resumo: 
- `heuristic_preprocessing()`: antecipa o que pode ser inferido sem o modelo
- `heuristic_update()`: permite que o sistema aprenda continuamente com novas extra√ß√µes, tornando-o mais eficiente conforme mais documentos s√£o processados.

#### Workflow

Os fluxogramas abaixo demonstram como os m√©todos `heuristic_preprocessing()` e `heuristic_update()`, respectivamente, funcionam.
 
```mermaid
flowchart LR
    A[Entrada: label, esquema de requisi√ß√£o e matriz do PDF] --> B
    B{Label presente na cache?}

    B -->|N√£o| C[Retorna dicion√°rio vazio]
    B -->|Sim| D

    D[Para cada chave presente no esquema de requisi√ß√£o] --> E
    E{H√° heur√≠sticas para a chave?}
    E -->|N√£o| G[Nada √© feito e muda para a pr√≥xima chave]
    E -->|Sim| F

    F[Para cada heur√≠stica presente na chave] --> H
    H{Acessa elemento na matriz do PDF com a posi√ß√£o armazenada pela heur√≠stica}

    H -->|Acesso inv√°lido| I[Heur√≠stica n√£o aplic√°vel. Muda para a pr√≥xima heur√≠stica]
    H -->|Acesso v√°lido| J

    J{Tipo armazenado pela her√≠stica compat√≠vel com o tipo do elemento acessado?}
    J -->|N√£o| K[Heur√≠stica n√£o aplic√°vel. Muda para a pr√≥xima heur√≠stica]
    J -->|Sim| L[Preenche par chave-valor no dicion√°rio a ser retornado, onde chave = chave da requisi√ß√£o e valor = elemento acessado no PDF. Muda para a pr√≥xima chave]
```

```mermaid
flowchart LR
    A[Entrada: label, resultado da infer√™ncia e matriz do PDF] --> B{Label existe na cache?}

    B -->|N√£o| C[Criar entrada vazia para a label na cache]
    B -->|Sim| D

    C --> D

    D[Iterar sobre chave, valor do resultado] --> E{Valor vazio ou nulo?}
    E -->|Sim| F[Ignorar valor]
    E -->|N√£o| H

    H{Chave j√° existe na cache para esta label?}
    H -->|N√£o| I[Inicializar estrutura da chave]
    H -->|Sim| J

    I --> J

    J[Atualizar estat√≠sticas da chave. Ex.: tipo predominante, contagem, exemplos representativos] --> K{Localizar posi√ß√£o do valor no PDF}

    K -->|N√£o encontrado| L[Encerrar atualiza√ß√£o para esta chave]
    K -->|Encontrado| M{Existe heur√≠stica para esta posi√ß√£o e tipo?}

    M -->|Sim| N[Incrementar match_count e atualizar m√©tricas]
    M -->|N√£o| O[Adicionar nova heur√≠stica]

    N --> P[Reordenar heur√≠sticas por match_count]
    O --> P

    P[Manter apenas as N heur√≠sticas mais fortes]
```

### üí¨ Altern√¢ncia de prompt

Conforme dito anteriormente, √© evidente o *trade-off* entre passar o PDF nativo e pass√°-lo como uma representa√ß√£o textual estruturada no prompt: a extra√ß√£o via PDF nativo tende a ser mais precisa, custosa e lenta e a extra√ß√£o baseada na matriz textual √© mais barata e r√°pida, mas pode ser menos fiel ao conte√∫do original.

A seguinte estrat√©gia foi utilizada para atacar esse desafio: para os casos em que a heur√≠stica n√£o p√¥de contribuir significativamente com o preenchimento do request_schema (quando o percentual de chaves preenchidas pela heur√≠stica para um determinado documento foi menor ou igual a um limiar predefinido - 50% no c√≥digo, valor pode ser ajustado) o sistema opta por utilizar a extra√ß√£o baseada no PDF nativo. Assim, al√©m de garantir maior precis√£o, tamb√©m permite atualizar a heur√≠stica com dados mais precisos e confi√°veis.

Nos casos em que o programa opta por utilizar a representa√ß√£o textual no prompt, al√©m de enviar o esquema de extra√ß√£o em YAML, tamb√©m s√£o inseridos exemplos previamente observados pela heur√≠stica para cada chave. Esses exemplos n√£o s√£o utilizados como valores fixos, mas como pistas sem√¢nticas para auxiliar o modelo - uma vez que essa abordagem tende a ser mais imprecisa. Em outras palavras, caso a heur√≠stica j√° tenha visto valores associados √†quela mesma chave em documentos da mesma label, tais valores servem como sinaliza√ß√£o do formato esperado, da terminologia utilizada ou da forma como aquela informa√ß√£o costuma aparecer.

Essa **abordagem h√≠brida** tenta explorar o melhor dos dois mundos: prioriza custo e efici√™ncia quando h√° hist√≥rico e conhecimento acumulado para aquela label, enquanto recorre ao PDF nativo para maximizar precis√£o justamente nos casos em que o risco de erro ou ambiguidade √© maior.



## üë®üèª‚Äçüíª Como usar

1. Clone o reposit√≥rio
    ```bash
    git clone https://github.com/joaoloss/ia-fellowship.git
    cd ia-fellowship
    ```

2. Crie um `.env`
    ```
    OPENAI_API_KEY=<sua-chave-api>
    ```

3. Inicialize o ambiente com [uv](https://docs.astral.sh/uv/)
    ```bash
    uv init
    uv sync
    ```

    Obs.: caso esteja utilizando o reposit√≥rio pela primeira vez, o uv criar√° automaticamente o ambiente isolado e instalar√° todas as depend√™ncias definidas no `pyproject.toml`.

4. Execu√ß√£o do Programa

    O programa pode ser utilizado de duas maneiras: via linha de comando (**CLI**) ou via interface gr√°fica (**UI**).

    - **CLI mode**
        ```bash
        uv run main.py [-h] [--verbose {debug,info,warning,error,tqdm}] [--input-json INPUT_JSON]
        ```

    - `--verbose`: N√≠vel de detalhamento dos logs. Pode ser: debug, info, warning, error ou tqdm (default: info).

    - `--input-json`: Nome do arquivo JSON de entrada quando executado em modo CLI (default: dataset.json).

        Exemplo:
        ```bash
        uv run main.py --verbose tqdm --input-json input.json
        ```
    
    - **UI mode**
        ```bash
        uv run streamlit run main.py  -- --streamlit 
        ```

        Em seguida acesse `http://localhost:8501` no navegador.

    Ao executar o programa via interface gr√°fica (**UI**), al√©m do processamento padr√£o, a aplica√ß√£o apresenta **estat√≠sticas e visualiza√ß√µes interativas** relacionadas ao processo de extra√ß√£o ‚Äî incluindo tempo de execu√ß√£o, custo estimado e desempenho da heur√≠stica.

## üî¢ Entrada e sa√≠da

### Entrada

Os arquivos PDF referenciados pelo JSON de entrada devem estar na pasta `files`. Al√©m disso o JSON de entrada deve seguir o seguinte padr√£o:
```json
[
    {
        "label": "carteira_oab",
        "extraction_schema": {
            "nome": "Nome do profissional, normalmente no canto superior esquerdo da imagem",
            "inscricao": "N√∫mero de inscri√ß√£o do profissional",
            "seccional": "Seccional do profissional",
            "subsecao": "Subse√ß√£o √† qual o profissional faz parte",
            "categoria": "Categoria, pode ser ADVOGADO, ADVOGADA, SUPLEMENTAR, ESTAGIARIO, ESTAGIARIA",
            "endereco_profissional": "Endere√ßo do profissional",
            "telefone_profissional": "Telefone do profissional",
            "situacao": "Situa√ß√£o do profissional, normalmente no canto inferior direito."
        },
        "pdf_path": "oab_1.pdf"
    }
]
```

### Sa√≠da

1. `results_<time-stamp>.json`: arquivo contendo o resultado do processamento juntamente com dados estat√≠sticos.

    Exemplo:
    ```json
    [
        {
            "extraction_schema": {
                "nome": "luis filipe araujo amaral",
                "inscricao": "101943",
                "seccional": "pr",
                "subsecao": "conselho seccional - paran√°",
                "categoria": "suplementar",
                "endereco_profissional": "avenida paulista, n¬∫ 2300 andar pilotis, bela vista s√£o paulo - sp\n\n01310300",
                "situacao": "situa√ß√£o regular"
            },
            "metadata": {
                "pdf_path": "oab_2.pdf",
                "label": "carteira_oab",
                "version_used": "text_based",
                "latency_seconds": 2.3,
                "total_tokens": 611,
                "input_tokens": 562,
                "output_tokens": 49,
                "cached_tokens": 0,
                "reasoning_tokens": 0,
                "estimated_cost_usd": "2.385000e-04",
                "heuristic_hits": [
                    "nome",
                    "inscricao",
                    "seccional",
                    "subsecao",
                    "categoria"
                ]
            }
        }
    ]
    ```

2. `debug_outputs/`: cont√©m artefatos auxiliares para depura√ß√£o, incluindo a representa√ß√£o matricial dos PDFs e um JSON com o estado final da cache de heur√≠sticas aprendidas durante o processamento.

## üß© Melhorias e limita√ß√µes reconhecidas

Como o algoritmo √© apenas um prot√≥tipo, √© importante pontuar limita√ß√µes/melhorias reconhecidas:

1. A vers√£o atual da heur√≠stica constru√≠da **n√£o identifica chaves ausentes**, o que aumenta a depend√™ncia do modelo de linguagem. Vers√µes futuras poderiam contornar esse problema.
2. **Aus√™ncia de paralelismo/multithreading**: adicionar essa feature √© um desafio que, infelizmente, n√£o p√¥de ser solucionado por quest√£o de prazo. Contudo, h√° alguns problemas que tornam a inser√ß√£o dessa feature n√£o trivial:
   1. Problema de sincronismo: ao processar m√∫ltiplos PDFs em paralelo, a ordem de processamento deixa de ser garantida, ou seja, a ordem de sa√≠da pode n√£o corresponder √† ordem de entrada.
   2. Efetividade reduzida da heur√≠stica: a heur√≠stica depende do ac√∫mulo progressivo de informa√ß√µes ‚Äî quanto mais documentos s√£o processados, melhor ela fica. Entretanto, com m√∫ltiplas threads, documentos que s√£o processados logo no in√≠cio podem n√£o se beneficiar da heur√≠stica simplesmente porque ela ainda n√£o foi atualizada por outras threads. 
   
        Uma poss√≠vel solu√ß√£o seria manter o processamento sequencial durante um determinado per√≠odo ou at√© que um n√∫mero m√≠nimo de documentos tenha sido processado.
3. A heur√≠stica est√° fortemente ligada √† identifica√ß√£o de padr√µes de layout presentes nos documentos. Embora seja capaz de armazenar e reconhecer m√∫ltiplas varia√ß√µes desses padr√µes, seu desempenho depende diretamente da recorr√™ncia entre os PDFs de uma mesma label. Quanto mais est√°veis forem esses padr√µes, maior tende a ser a cobertura heur√≠stica.
4. Como a heur√≠stica √© adaptativa (aprendizado acumulativo) para extra√ß√µes isoladas o resultado n√£o √© otimizado.
5. O tratamento de erros e inconsist√™ncias ainda pode ser aprimorado, especialmente em cen√°rios n√£o previstos ou de entrada inv√°lida.