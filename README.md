# AI Fellowship Data Repository

- [AI Fellowship Data Repository](#ai-fellowship-data-repository)
  - [ðŸ“ DescriÃ§Ã£o do problema](#-descriÃ§Ã£o-do-problema)
  - [ðŸ’» Stack](#-stack)
  - [ðŸ’¡ EstratÃ©gia](#-estratÃ©gia)
    - [ðŸ¤– LLM: reduzindo latÃªncia e custos](#-llm-reduzindo-latÃªncia-e-custos)
    - [ðŸ¤¯ HeurÃ­stica](#-heurÃ­stica)
      - [Pressupostos adotados](#pressupostos-adotados)
      - [Cache](#cache)
      - [Workflow](#workflow)


Esse repositÃ³rio contÃ©m um projeto desenvolvido durante o processo seletivo para o fellowship promovido pela empresa [Enter](https://www.getenter.ai/).

## ðŸ“ DescriÃ§Ã£o do problema

O desafio proposto gira em torno do problema de extraÃ§Ã£o eficiente de pares chave-valor a partir de documentos desestruturados. Conforme apontado por [esse paper](https://arxiv.org/abs/2405.00505), por exemplo, *Key-Value Pair Extraction* Ã© uma tarefa crÃ­tica cuja soluÃ§Ã£o eficiente permanece em aberto.

## ðŸ’» Stack

Por questÃµes de familiaridade e agilidade no desenvolvimento/prototipaÃ§Ã£o, optou-se pela linguagem Python.

AlÃ©m disso, como modelo de linguagem (LLM), utilizou-se o [gpt-5-mini](https://platform.openai.com/docs/models/gpt-5-mini) da OpenAI.

## ðŸ’¡ EstratÃ©gia

### ðŸ¤– LLM: reduzindo latÃªncia e custos

**Nota**: no contexto de inferÃªncia de modelos de linguagem: `tokens consumidos = custo`. Portanto, um aumento/reduÃ§Ã£o no nÃºmero de tokens implica um aumento/reduÃ§Ã£o proporcional no custo final.

Sabendo que a interaÃ§Ã£o com um LLM seria uma peÃ§a fundamental e inegociÃ¡vel, o primeiro passo tomado durante o desenvolvimento foi testar formas de diminuir custo e latÃªncia (uma vez que chamadas a LLMs costumam ser o gargalo operacional e financeiro da operaÃ§Ã£o):

1. Percebendo que a tarefa de identificar pares chave-valor nÃ£o demanda uma linha de raciocÃ­nio muito elaborada, o primeiro teste feito foi retirar (ou, praticamente retirar) a feature de `reasoning` do modelo, setando `reasoning={"effort": "minimal"}` (os testes foram feitos passando o PDF como texto via prompt). 
    - Resultado: de ~20s para ~3s (**7x menos**) e de ~1600 tokens totais para ~400 (**4x menos**), sendo que o resultado permaneceu satisfatÃ³rio.
    - Obs.: quando `effort` nÃ£o Ã© especificado, o valor padrÃ£o Ã© `medium`.

2. Para evitar formatos de saÃ­da indesejados (o que geraria problemas desnecessÃ¡rios de JSON parsing), utilizou-se a feature de [Structured model outputs](https://platform.openai.com/docs/guides/structured-outputs), garantindo que o modelo sempre responderia conforme o modelo JSON estabelecido (utilizando a lib. `pydantic`).

3. Como uma tentativa de "enguxar" ainda mais o prompt, o esquema de entrada foi passado na estrutura YAML, que utiliza menos tokens que o formato JSON - o resultado nÃ£o foi significativo, um vez que essa Ã© um estrtÃ©gia crÃ­tica para cenÃ¡rios onde o JSON passado no prompt Ã© extremamente longo, o que nÃ£o Ã© o caso mÃ©dio do desafio.

4. Por fim, testou-se passar o PDF de entrada de duas formas:
    1. Utilizando a feature de [File inputs](https://platform.openai.com/docs/guides/pdf-files?api-mode=responses) via base64, o que inevitavelmente aumenta custo e latÃªncia - uma vez que: "To help models understand PDF content, we put into the model's context both extracted text and an image of each pageâ€”regardless of whether the page includes images.", OpenAI.
    2. Utilizando apenas texto via engenharia de prompt. Realizar isso Ã© complicado, uma vez que o layout desempenha um papel fundamental. Para contornar esse problema utilizou-se a seguinte estratÃ©gia: alÃ©m de passar o texto "cru" e corrido, tambÃ©m foi fornecido ao modelo um esquema que lhe permitiria entender o layout do arquivo original (aqui, comeÃ§a a entrar a heurÃ­stica utilizada, que serÃ¡ detalhada no prÃ³ximo tÃ³pico) atravÃ©s de uma matriz. O exemplo abaixo mostra os dos dois formatos para o arquivo `oab_1.pdf`:
        - Texto corrido:
            ```none
            joana d'arc inscriÃ§Ã£o seccional subseÃ§Ã£o 101943 pr conselho seccional - paranÃ¡ suplementar endereÃ§o profissional avenida paulista, nÂº 2300 andar pilotis, bela vista sÃ£o paulo - sp 01310300 telefone profissional situaÃ§Ã£o regular
            ```
        - Formato estruturado:
            ```none
            Row 1: joana d'arc
            Row 2: inscriÃ§Ã£o | seccional | subseÃ§Ã£o
            Row 3: 101943 | pr | conselho seccional - paranÃ¡
            Row 4: suplementar
            Row 5: endereÃ§o profissional
            Row 6: avenida paulista, nÂº 2300 andar pilotis, bela vista
            Row 7: sÃ£o paulo - sp
            Row 8: 01310300
            Row 9: telefone profissional
            Row 10: situaÃ§Ã£o regular
            ```
        Apesar de modelos de linguagem serem, em essÃªncia, orientados a texto e nÃ£o apresentarem desempenho ideal em dados tabulares, observou-se uma melhora significativa nos resultados quando as informaÃ§Ãµes foram estruturadas em tabela/matriz, em comparaÃ§Ã£o ao uso do texto corrido sozinho. Obviamente isso acabou resultando em um pequeno aumento de latÃªncia e tokens consumidos.
    
    **Resultados**: enviar o arquivo PDF para o LLM (via base64), em vez do texto extraÃ­do do PDF no prompt, resultou em aproximadamente **2x mais tempo**, **2x mais tokens**. Contudo, durante os experimentos, percebeu-se que os resultados foram um pouco inferiores e menos consistentes. Exemplos:
    - Para a chave `"situacao"` dentro de `"label": "carteira_oab"`: em alguns casos, o modelo retornou apenas `"regular"`, enquanto em outros retornou `"situaÃ§Ã£o regular"`. AlÃ©m disso, para a chave `"endereco_profissional"` dentro da mesma categoria: partes finais do endereÃ§o foram ocasionalmente omitidas â€” como, por exemplo, o CEP.

    Abaixo tem-se o resultado para `oab_1.pdf` utiliando as duas abordagens:
    - Passando o arquivo:
    ```json
    "extraction_schema": {
            "nome": "JOANA D'ARC",
            "inscricao": "101943",
            "seccional": "PR",
            "subsecao": "CONSELHO SECCIONAL - PARANÃ",
            "categoria": "SUPLEMENTAR",
            "endereco_profissional": "AVENIDA PAULISTA, NÂº 2300 andar Pilotis, Bela Vista SÃƒO PAULO - SP 01310300",
            "telefone_profissional": null,
            "situacao": "SITUAÃ‡ÃƒO REGULAR"
        },
        "latency_seconds": 7.63,
        "total_tokens": 1573,
        "input_tokens": 1471,
        "output_tokens": 102
    ```
    - Usando apenas texto:
    ```json
    "extraction_schema": {
            "nome": "joana d'arc",
            "inscricao": "101943",
            "seccional": "pr",
            "subsecao": "conselho seccional - paranÃ¡",
            "categoria": "SUPLEMENTAR",
            "endereco_profissional": "avenida paulista, nÂº 2300 andar pilotis, bela vista",
            "telefone_profissional": null,
            "situacao": "regular"
        },
        "latency_seconds": 2.47,
        "total_tokens": 796,
        "input_tokens": 713,
        "output_tokens": 83
    ````

### ðŸ¤¯ HeurÃ­stica

#### Pressupostos adotados

1. Conjunto definido de layouts por label.
    - Assumiu-se que documentos com mesma label tendem a possuir um conjunto de layouts padrÃ£o. Ou seja, para uma mesma label existe um conjunto de configuraÃ§Ãµes a partir das quais os dados estÃ£o dispostos.
2. Mesma chave, mesmo tipo.
    - Assumiu-se valores de labels e chaves iguais possuem o mesmo tipo/formato.
    - Exemplo: dada uma label, uma chave `nome` sempre conterÃ¡ uma string, uma chave `data` sempre conterÃ¡ um valor no formato de data, uma chave `valor_total` sempre conterÃ¡ um valor numÃ©rio, etc..
3. LLM acerta.
    - Assume-se que o resultado gerado pela LLM estÃ¡ correto.

#### Cache

A cache Ã© um dicionÃ¡rio cujos valores sÃ£o preenchidos de forma adaptativa ao longo do processamento dos PDFs. Sua estrutura segue trÃªs nÃ­veis:

1. **NÃ­vel 1**: chaves correspondendo Ã s *labels* dos documentos (ex.: `carteira_oab`, `tela_sistema`, etc.), permitindo que heurÃ­sticas sejam especializadas por tipo de documento.

2. **NÃ­vel 2**: cada label possui um dicionÃ¡rio como valor, cujas chaves correspondem Ã s *keys* do esquema.

3. **NÃ­vel 3**: cada key possui um dicionÃ¡rio como valor, cujas chaves sÃ£o:
    1. `count`, que armazena a quantidade total de vezes que a key foi solicitada em um esquema de requisiÃ§Ã£o, e
    2. `heuristics`, que corresponde a uma lista de heurÃ­sticas aprendidas.

4. **NÃ­vel 4**: cada heurÃ­stica Ã© um dicionÃ¡rio cujas chaves sÃ£o:
    1. `type`: tipo de dado (ver mÃ³dulo `utils.type_resolution.py`),
    2. `position`: posiÃ§Ã£o do valor na representaÃ§Ã£o matricial do conteÃºdo do PDF (ver mÃ³dulo `utils.pdf2mat.py`),
    3. `match_count`: nÃºmero de vezes que essa heurÃ­stica foi usada,
    4. Se o tipo for `string`, hÃ¡ tambÃ©m a chave `mean_length`: armazena um float com o tamanho mÃ©dio acumulado dos valores da chave.

A cada nova extraÃ§Ã£o, o mÃ©todo `heuristic_update()` atualiza o cache reforÃ§ando heurÃ­sticas existentes ou adicionando novas, priorizando as que apresentam maior frequÃªncia de acertos.
Posteriormente, no processo de prÃ©-inferÃªncia (mÃ©todo `heuristic_preprocessing()`), essas heurÃ­sticas sÃ£o utilizadas para preencher automaticamente valores do esquema de requisiÃ§Ã£o, reduzindo a quantidade de dados solicitados ao modelo de linguagem.

#### Workflow

Com base nos pressupostos listados, a heurÃ­stica criada segue os seguintes passos:
```mermaid
flowchart LR
    A[Inicio da Extracao] --> B[Recebe label e schema]
    B --> C{Ha heuristica em cache para este label?}
    
    C -->|Nao| D[Nenhuma chave preenchida]
    C -->|Sim| E[Itera pelas chaves do schema]
    
    E --> F{Ha heuristica para a chave?}
    F -->|Nao| G[Sem preenchimento]
    F -->|Sim| H[Tenta localizar posicao na matriz do PDF]
    
    H --> I{Tipo e valor sao compativeis?}
    I -->|Nao| G
    I -->|Sim| J[Preenche campo no partial_result]
    
    J --> K[Retorna partial_result]
    
    %% Atualizacao do cache
    K --> L[Recebe resultado final da extracao]
    L --> M[Para cada chave com valor valido]
    M --> N[Obtem posicao do valor no PDF]
    N --> O[Resolve tipo do valor]
    O --> P[Atualiza cache para label e chave]
    
    P --> Q[Ordena heuristicas por]
```
