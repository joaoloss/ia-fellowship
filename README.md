# AI Fellowship Data Repository

- [AI Fellowship Data Repository](#ai-fellowship-data-repository)
  - [ğŸ“ DescriÃ§Ã£o do problema](#-descriÃ§Ã£o-do-problema)
  - [ğŸ’» Stack](#-stack)
  - [ğŸ’¡ EstratÃ©gia](#-estratÃ©gia)
    - [ğŸ¤– LLM: reduzindo latÃªncia e custos](#-llm-reduzindo-latÃªncia-e-custos)
    - [ğŸ¤¯ HeurÃ­stica](#-heurÃ­stica)
      - [Pressupostos adotados](#pressupostos-adotados)
      - [Cache](#cache)
      - [Workflow](#workflow)
  - [ğŸ‘¨ğŸ»â€ğŸ’» Como usar](#-como-usar)
  - [ğŸ§© Melhorias reconhecidas](#-melhorias-reconhecidas)


Esse repositÃ³rio contÃ©m um projeto desenvolvido durante o processo seletivo para o fellowship promovido pela empresa [Enter](https://www.getenter.ai/).

## ğŸ“ DescriÃ§Ã£o do problema

O desafio proposto gira em torno do problema de extraÃ§Ã£o eficiente de pares chave-valor a partir de documentos desestruturados. Conforme apontado por [esse paper](https://arxiv.org/abs/2405.00505), por exemplo, *Key-Value Pair Extraction* Ã© uma tarefa crÃ­tica cuja soluÃ§Ã£o eficiente permanece em aberto.

## ğŸ’» Stack

Por questÃµes de familiaridade e agilidade no desenvolvimento/prototipaÃ§Ã£o, optou-se pela linguagem Python.

AlÃ©m disso, como modelo de linguagem (LLM), utilizou-se o [gpt-5-mini](https://platform.openai.com/docs/models/gpt-5-mini) da OpenAI.

## ğŸ’¡ EstratÃ©gia

### ğŸ¤– LLM: reduzindo latÃªncia e custos

**Nota**: no contexto de inferÃªncia de modelos de linguagem: `tokens consumidos = custo`. Portanto, um aumento/reduÃ§Ã£o no nÃºmero de tokens implica um aumento/reduÃ§Ã£o proporcional no custo final.

Sabendo que a interaÃ§Ã£o com um LLM seria uma peÃ§a fundamental e inegociÃ¡vel, o primeiro passo tomado durante o desenvolvimento foi testar formas de diminuir custo e latÃªncia (uma vez que chamadas a LLMs costumam ser o gargalo operacional e financeiro da operaÃ§Ã£o):

1. Percebendo que a tarefa de identificar pares chave-valor nÃ£o demanda uma linha de raciocÃ­nio muito elaborada, o primeiro teste feito foi retirar (ou, praticamente retirar) a feature de `reasoning` do modelo, setando `reasoning={"effort": "minimal"}` (os testes foram feitos passando o PDF como texto via prompt). 
    - Resultado: de ~20s para ~3s (**7x menos**) e de ~1600 tokens totais para ~400 (**4x menos**), sendo que o resultado permaneceu satisfatÃ³rio.
    - Obs.: quando `effort` nÃ£o Ã© especificado, o valor padrÃ£o Ã© `medium`.

2. Para evitar formatos de saÃ­da indesejados (o que geraria problemas desnecessÃ¡rios de JSON parsing), utilizou-se a feature de [Structured model outputs](https://platform.openai.com/docs/guides/structured-outputs), garantindo que o modelo sempre responderia conforme o modelo JSON estabelecido (utilizando a lib. `pydantic`).

3. Como uma tentativa de "enguxar" ainda mais o prompt, o esquema de entrada foi passado na estrutura YAML, que utiliza menos tokens que o formato JSON - o resultado nÃ£o foi significativo, um vez que essa Ã© um estrtÃ©gia crÃ­tica para cenÃ¡rios onde o JSON passado no prompt Ã© extremamente longo, o que nÃ£o Ã© o caso mÃ©dio do desafio.

4. Para aproveitar melhor o [Prompt caching](https://platform.openai.com/docs/guides/prompt-caching) (reduzindo custo e latÃªncia), o prompt foi organizado de forma que as seÃ§Ãµes estÃ¡veis permaneÃ§am no inÃ­cio, enquanto as partes variÃ¡veis sÃ£o colocadas ao final, reduzindo a quantidade de conteÃºdo que precisa ser recarregado a cada requisiÃ§Ã£o.

5. Por fim, testou-se passar o PDF de entrada de duas formas:
    1. Utilizando a feature de [File inputs](https://platform.openai.com/docs/guides/pdf-files?api-mode=responses) via base64, o que inevitavelmente aumenta custo e latÃªncia - uma vez que: "To help models understand PDF content, we put into the model's context both extracted text and an image of each pageâ€”regardless of whether the page includes images.", OpenAI.
    2. Utilizando apenas texto via engenharia de prompt. Realizar isso Ã© complicado, uma vez que o layout desempenha um papel fundamental. Para contornar esse problema foi fornecido ao modelo um esquema que lhe permite entender o layout do arquivo original (aqui, comeÃ§a a entrar a heurÃ­stica utilizada, que serÃ¡ detalhada no prÃ³ximo tÃ³pico) atravÃ©s de uma matriz. Exemplo para o arquivo `oab_1.pdf`:
   
            ```none
            Row 1: joana d'arc
            Row 2: inscriÃ§Ã£o | seccional | subseÃ§Ã£o
            Row 3: 101943 | pr | conselho seccional - paranÃ¡
            Row 4: suplementar
            Row 5: endereÃ§o profissional
            Row 6: avenida paulista, nÂº 2300 andar pilotis, bela vista
            Row 7: sÃ£o paulo - sp
            Row 8: 01310300
            Row 9: telefone profissional
            Row 10: situaÃ§Ã£o regular
            ```

        Apesar de modelos de linguagem serem, em essÃªncia, orientados a texto e nÃ£o apresentarem desempenho ideal em dados tabulares, observou-se uma melhora significativa nos resultados quando as informaÃ§Ãµes foram estruturadas em tabela/matriz, em comparaÃ§Ã£o ao uso do texto corrido sozinho. Obviamente isso acabou resultando em um pequeno aumento de latÃªncia e tokens consumidos.
    
    **Resultados**: enviar o arquivo PDF para o LLM (via base64), em vez do texto extraÃ­do do PDF no prompt, resultou em aproximadamente **2x mais tempo**, **2x mais tokens**. Contudo, durante os experimentos, percebeu-se que os resultados foram um pouco inferiores e menos consistentes. Exemplos:
    - Para a chave `"situacao"` dentro de `"label": "carteira_oab"`: em alguns casos, o modelo retornou apenas `"regular"`, enquanto em outros retornou `"situaÃ§Ã£o regular"`. AlÃ©m disso, para a chave `"endereco_profissional"` dentro da mesma categoria: partes finais do endereÃ§o foram ocasionalmente omitidas â€” como, por exemplo, o CEP.

    O tÃ³pico a seguir apresenta a abordagem adotada para lidar com esses problemas.

### ğŸ¤¯ HeurÃ­stica

#### Pressupostos adotados

1. Conjunto definido de layouts por label.
    - Assumiu-se que documentos com mesma label tendem a possuir um conjunto de layouts padrÃ£o. Ou seja, para uma mesma label existe um conjunto de configuraÃ§Ãµes a partir das quais os dados estÃ£o dispostos.
2. Mesma chave, mesmo tipo.
    - Assumiu-se valores de labels e chaves iguais possuem o mesmo tipo/formato.
    - Exemplo: dada uma label, uma chave `nome` sempre conterÃ¡ uma string, uma chave `data` sempre conterÃ¡ um valor no formato de data, uma chave `valor_total` sempre conterÃ¡ um valor numÃ©rio, etc..
3. LLM acerta.
    - Assume-se que o resultado gerado pela LLM quando alimentada com o arquivo PDF nativo estÃ¡ correto.

#### Cache

A cache Ã© um dicionÃ¡rio cujos valores sÃ£o preenchidos de forma adaptativa ao longo do processamento dos PDFs. Sua estrutura segue trÃªs nÃ­veis:

1. **NÃ­vel 1**: chaves correspondendo Ã s *labels* dos documentos (ex.: `carteira_oab`, `tela_sistema`, etc.), permitindo que heurÃ­sticas sejam especializadas por tipo de documento.

2. **NÃ­vel 2**: cada label possui um dicionÃ¡rio como valor, cujas chaves correspondem Ã s *keys* do esquema.

3. **NÃ­vel 3**: cada key possui um dicionÃ¡rio como valor, cujas chaves sÃ£o:
    1. `count`, que armazena a quantidade total de vezes que a key foi solicitada em um esquema de requisiÃ§Ã£o,
    2. `heuristics`, que corresponde a uma lista de heurÃ­sticas aprendidas (a ideia Ã© que cada heurÃ­stica seja Ãºtil para um layout especÃ­fico),
    3. `type`, que corresponde ao tipo predominante do valor correspondente e
    4. `example_values`, que corresponde a uma lista de valores prÃ©vios.

4. **NÃ­vel 4**: cada heurÃ­stica Ã© um dicionÃ¡rio cujas chaves sÃ£o:
    1. `position`: posiÃ§Ã£o do valor na representaÃ§Ã£o matricial do conteÃºdo do PDF (ver mÃ³dulo `utils.pdf2mat.py`),
    2. `match_count`: nÃºmero de vezes que essa heurÃ­stica foi usada,
    3. Se o tipo for `string`, hÃ¡ tambÃ©m a chave `mean_length`: armazena um float com o tamanho mÃ©dio acumulado dos valores da chave.

Exemplo da estrutura da cache:
```json
"carteira_oab": {
    "nome": {
        "count": 3,
        "heuristics": [
            {
                "position": [
                    0
                ],
                "match_count": 3,
                "mean_length": 11
            }
        ],
        "type": "string",
        "example_values": [
            "joana d'arc",
            "luis filipe araujo amaral",
            "son goku"
        ]
    },
    "inscricao": {
        "count": 3,
        "heuristics": [
            {
                "position": [
                    2,
                    0
                ],
                "match_count": 3
            }
        ],
        "type": "number",
        "example_values": [
            "101943"
        ]
    }
}
```

**Antes de realizar a chamada ao modelo** (gargalo do sistema em termos de custo e tempo) executa-se um prÃ©-processamento por meio do mÃ©todo `heuristic_preprocessing()`. Esse mÃ©todo utiliza a cache de heurÃ­sticas jÃ¡ aprendidas para tentar preencher automaticamente parte do esquema de extraÃ§Ã£o (`request_schema`) antes da inferÃªncia. Para cada chave do esquema, o mÃ©todo verifica se existem heurÃ­sticas previamente armazenadas para a label do documento atual e, se existir, tenta recuperar o valor correspondente consultando diretamente a matriz do PDF. Os valores recuperados sÃ£o armazenados em um dicionÃ¡rio parcial (`partial_result`), que representa os campos resolvidos apenas por heurÃ­stica, sem consulta ao modelo. Durante esse processo, o mÃ©todo tambÃ©m ajusta contadores internos e estatÃ­sticas de uso das heurÃ­sticas, reforÃ§ando aquelas que se mostram mais eficazes.

**ApÃ³s a inferÃªncia do modelo**, o mÃ©todo `heuristic_update()` Ã© responsÃ¡vel por atualizar a cache com os novos resultados obtidos. Ele registra o valor retornado, determina seu tipo, coleta exemplos representativos e identifica a posiÃ§Ã£o do valor no PDF, transformando esse conhecimento em novas heurÃ­sticas. Se uma heurÃ­stica existente jÃ¡ corresponder ao valor observado, sua frequÃªncia de acerto Ã© incrementada; caso contrÃ¡rio, uma nova heurÃ­stica Ã© adicionada. O conjunto Ã© entÃ£o reordenado para priorizar heurÃ­sticas mais consistentes, mantendo apenas as mais relevantes para uso futuro.

Em resumo: 
- `heuristic_preprocessing()`: antecipa o que pode ser inferido sem o modelo
- `heuristic_update()`: permite que o sistema aprenda continuamente com novas extraÃ§Ãµes, tornando-o mais eficiente conforme mais documentos sÃ£o processados.

#### Workflow

Os fluxogramas abaixo demonstram como os mÃ©todos `heuristic_preprocessing()` e `heuristic_update()`, respectivamente, funcionam.
 
```mermaid
flowchart LR
    A[Entrada: label, esquema de requisiÃ§Ã£o e matriz do PDF] --> B
    B{Label presente na cache?}

    B -->|NÃ£o| C[Retorna dicionÃ¡rio vazio]
    B -->|Sim| D

    D[Para cada chave presente no esquema de requisiÃ§Ã£o] --> E
    E{HÃ¡ heurÃ­sticas para a chave?}
    E -->|NÃ£o| G[Nada Ã© feito e muda para a prÃ³xima chave]
    E -->|Sim| F

    F[Para cada heurÃ­stica presente na chave] --> H
    H{Acessa elemento na matriz do PDF com a posiÃ§Ã£o armazenada pela heurÃ­stica}

    H -->|Acesso invÃ¡lido| I[HeurÃ­stica nÃ£o aplicÃ¡vel. Muda para a prÃ³xima heurÃ­stica]
    H -->|Acesso vÃ¡lido| J

    J{Tipo armazenado pela herÃ­stica compatÃ­vel com o tipo do elemento acessado?}
    J -->|NÃ£o| K[HeurÃ­stica nÃ£o aplicÃ¡vel. Muda para a prÃ³xima heurÃ­stica]
    J -->|Sim| L[Preenche par chave-valor no dicionÃ¡rio a ser retornado, onde chave = chave da requisiÃ§Ã£o e valor = elemento acessado no PDF. Muda para a prÃ³xima chave]
```

```mermaid
flowchart LR
    A[Entrada: label, resultado da inferÃªncia e matriz do PDF] --> B{Label existe na cache?}

    B -->|NÃ£o| C[Criar entrada vazia para a label na cache]
    B -->|Sim| D

    C --> D

    D[Iterar sobre chave, valor do resultado] --> E{Valor vazio ou nulo?}
    E -->|Sim| F[Ignorar valor]
    E -->|NÃ£o| H

    H{Chave jÃ¡ existe na cache para esta label?}
    H -->|NÃ£o| I[Inicializar estrutura da chave]
    H -->|Sim| J

    I --> J

    J[Atualizar estatÃ­sticas da chave. Ex.: tipo predominante, contagem, exemplos representativos] --> K{Localizar posiÃ§Ã£o do valor no PDF}

    K -->|NÃ£o encontrado| L[Encerrar atualizaÃ§Ã£o para esta chave]
    K -->|Encontrado| M{Existe heurÃ­stica para esta posiÃ§Ã£o e tipo?}

    M -->|Sim| N[Incrementar match_count e atualizar mÃ©tricas]
    M -->|NÃ£o| O[Adicionar nova heurÃ­stica]

    N --> P[Reordenar heurÃ­sticas por match_count]
    O --> P

    P[Manter apenas as N heurÃ­sticas mais fortes]
```

## ğŸ‘¨ğŸ»â€ğŸ’» Como usar

1. Clone o repositÃ³rio
```bash
https://github.com/joaoloss/ia-fellowship.git
cd ia-fellowship
```

2. 
```bash
uv init
```

2. 

## ğŸ§© Melhorias reconhecidas

Como o algoritmo Ã© apenas um protÃ³tipo, Ã© importante pontuar limitaÃ§Ãµes/melhorias reconhecidas:

1. A versÃ£o atual da heurÃ­stica construÃ­da **nÃ£o identifica chaves ausentes**, o que aumenta a dependÃªncia do modelo de linguagem. VersÃµes futuras poderiam contornar esse problema.
2. **AusÃªncia de paralelismo/multithreading**: adicionar essa feature Ã© um desafio que, infelizmente, nÃ£o pÃ´de ser solucionado por questÃ£o de prazo. Contudo, hÃ¡ alguns problemas que tornam a inserÃ§Ã£o dessa feature nÃ£o trivial:
   1. Problema de sincronismo: ao processar mÃºltiplos PDFs em paralelo, a ordem de processamento deixa de ser garantida, ou seja, a ordem de saÃ­da pode nÃ£o corresponder Ã  ordem de entrada.
   2. Efetividade reduzida da heurÃ­stica: a heurÃ­stica depende do acÃºmulo progressivo de informaÃ§Ãµes â€” quanto mais documentos sÃ£o processados, melhor ela fica. Entretanto, com mÃºltiplas threads, documentos que sÃ£o processados logo no inÃ­cio podem nÃ£o se beneficiar da heurÃ­stica simplesmente porque ela ainda nÃ£o foi atualizada por outras threads. 
   
        Uma possÃ­vel soluÃ§Ã£o seria manter o processamento sequencial durante um determinado perÃ­odo ou atÃ© que um nÃºmero mÃ­nimo de documentos tenha sido processado.